"""
é£ä¹¦æ–‡æ¡£æ ¼å¼åŒ–å™¨

ä½¿ç”¨é£ä¹¦æ–‡æ¡£å—ç»“æ„ï¼Œä¼˜åŒ–ç®€æŠ¥çš„å¯è¯»æ€§ï¼š
- æ ‡é¢˜å±‚çº§
- é«˜äº®å—ï¼ˆCalloutï¼‰
- åˆ†å‰²çº¿
- è¡¨æ ¼
- åˆ—è¡¨
"""

import sys
sys.path.insert(0, "/workspace/openclaw/skills/feishu-doc-operations/scripts")

from feishu_doc_operations import obtainIdaasClientId, obtainIdaasClientSecret, obtainUserName, main as feishu_doc_main


def feishu_operation(params: dict) -> dict:
    """å°è£…é£ä¹¦æ–‡æ¡£æ“ä½œ"""
    action = params.get('action')
    
    if action == 'create':
        return feishu_doc_main({
            'action': 'write',
            'title': params.get('title'),
            'content': params.get('content'),
            'folder_token': params.get('folder_token'),
            'client_id': obtainIdaasClientId(),
            'client_secret': obtainIdaasClientSecret(),
            'userName': obtainUserName()
        })
    
    return {'code': -1, 'msg': f'Unsupported action: {action}'}
from datetime import datetime
from typing import Optional


CATEGORY_INFO = {
    'ai': {'name': 'AIç±»', 'icon': 'ğŸ¤–', 'color': 'blue'},
    'robotics': {'name': 'æœºå™¨äººç±»', 'icon': 'ğŸ¦¾', 'color': 'purple'},
    'embodied_ai': {'name': 'å…·èº«æ™ºèƒ½ç±»', 'icon': 'ğŸ‘“', 'color': 'indigo'},
    'semiconductor': {'name': 'åŠå¯¼ä½“è¡Œä¸šç±»', 'icon': 'ğŸ’¾', 'color': 'orange'},
    'auto': {'name': 'æ±½è½¦ç±»', 'icon': 'ğŸš—', 'color': 'green'},
    'health': {'name': 'å¥åº·åŒ»ç–—ç±»', 'icon': 'ğŸ¥', 'color': 'red'},
    'economy': {'name': 'ç»æµæ”¿ç­–ç±»', 'icon': 'ğŸ“Š', 'color': 'yellow'},
    'business': {'name': 'å•†ä¸šç§‘æŠ€ç±»', 'icon': 'ğŸ’¼', 'color': 'turquoise'},
    'politics': {'name': 'æ”¿æ²»æ”¿ç­–ç±»', 'icon': 'ğŸ›ï¸', 'color': 'grey'},
    'investment': {'name': 'æŠ•èµ„è´¢ç»ç±»', 'icon': 'ğŸ“ˆ', 'color': 'wathet'},
    'consumer_electronics': {'name': 'æ¶ˆè´¹ç”µå­ç±»', 'icon': 'ğŸ“±', 'color': 'carmine'},
    'key_people': {'name': 'å…³é”®äººç‰©å‘è¨€', 'icon': 'ğŸ¤', 'color': 'violet'},
}


def format_article_card(article: dict, index: int, cat_info: dict) -> str:
    """æ ¼å¼åŒ–å•ç¯‡æ–‡ç« ä¸ºå¡ç‰‡æ ·å¼çš„æ–‡æœ¬å—"""
    
    # ä½¿ç”¨ Markdown æ ¼å¼ï¼Œé£ä¹¦ä¼šè‡ªåŠ¨æ¸²æŸ“
    parts = []
    
    # æ–‡ç« æ ‡é¢˜ï¼ˆåŒè¯­ï¼‰
    parts.append(f"### {index}. {article['title_original']}")
    if article['title_zh'] != article['title_original']:
        parts.append(f"**{article['title_zh']}**")
    parts.append("")
    
    # å…ƒä¿¡æ¯è¡Œ
    meta = f"ğŸ“° **{article['source']}** Â· ğŸ• {article['published_at']}"
    if article.get('mentioned_people'):
        meta += f" Â· ğŸ‘¤ {', '.join(article['mentioned_people'])}"
    parts.append(meta)
    parts.append("")
    
    # æ‘˜è¦ï¼ˆä½¿ç”¨å¼•ç”¨æ ¼å¼ï¼‰
    parts.append("**ğŸ“‹ æ‘˜è¦**")
    parts.append("")
    # å°†æ‘˜è¦åˆ†æ®µ
    summary_paragraphs = article['summary_zh'].split('\n\n')
    for para in summary_paragraphs[:3]:  # æœ€å¤š3æ®µ
        parts.append(f"> {para.strip()}")
        parts.append(">")
    parts.append("")
    
    # å…³é”®è¦ç‚¹ï¼ˆä½¿ç”¨åˆ—è¡¨ï¼‰
    if article.get('key_points'):
        parts.append("**ğŸ”‘ å…³é”®è¦ç‚¹**")
        for point in article['key_points'][:4]:  # æœ€å¤š4ç‚¹
            parts.append(f"â€¢ {point}")
        parts.append("")
    
    # å½±å“åˆ†æï¼ˆä½¿ç”¨é«˜äº®å—æ ·å¼ï¼‰
    if article.get('impact_analysis'):
        parts.append("**ğŸ“ˆ å½±å“åˆ†æ**")
        parts.append(f"ğŸ’¡ {article['impact_analysis'][:300]}...")
        parts.append("")
    
    # é“¾æ¥
    parts.append(f"ğŸ”— [é˜…è¯»åŸæ–‡]({article['url']})")
    parts.append("")
    parts.append("---")
    parts.append("")
    
    return '\n'.join(parts)


def format_overview(articles_by_category: dict, date_str: str) -> str:
    """æ ¼å¼åŒ–ä»Šæ—¥æ¦‚è§ˆ"""
    
    parts = []
    
    # æ ‡é¢˜
    parts.append(f"# ğŸ“° å…¨çƒç§‘æŠ€ç®€æŠ¥")
    parts.append("")
    parts.append(f"## ğŸ“… {date_str}")
    parts.append("")
    parts.append("---")
    parts.append("")
    
    # ç»Ÿè®¡å¡ç‰‡
    total = sum(len(articles) for articles in articles_by_category.values())
    categories_with_content = [(cat, articles) for cat, articles in articles_by_category.items() if articles]
    
    parts.append("## ğŸ“Š ä»Šæ—¥æ¦‚è§ˆ")
    parts.append("")
    parts.append(f"**å…±è®¡ {total} æ¡æ–°é—»ï¼Œè¦†ç›– {len(categories_with_content)} ä¸ªç±»åˆ«**")
    parts.append("")
    
    # åˆ†ç±»ç»Ÿè®¡è¡¨æ ¼
    parts.append("| ç±»åˆ« | æ•°é‡ | å¤´æ¡ |")
    parts.append("|------|------|------|")
    
    for cat, articles in categories_with_content:
        info = CATEGORY_INFO.get(cat, {'name': cat, 'icon': 'ğŸ“Œ'})
        headline = articles[0]['title_zh'][:30] + '...' if len(articles[0]['title_zh']) > 30 else articles[0]['title_zh']
        parts.append(f"| {info['icon']} {info['name']} | {len(articles)} | {headline} |")
    
    parts.append("")
    parts.append("---")
    parts.append("")
    
    return '\n'.join(parts)


def format_category_section(category: str, articles: list) -> str:
    """æ ¼å¼åŒ–å•ä¸ªåˆ†ç±»çš„æ–°é—»"""
    
    if not articles:
        return ""
    
    info = CATEGORY_INFO.get(category, {'name': category, 'icon': 'ğŸ“Œ', 'color': 'grey'})
    
    parts = []
    parts.append(f"## {info['icon']} {info['name']}")
    parts.append("")
    
    for i, article in enumerate(articles, 1):
        parts.append(format_article_card(article, i, info))
    
    return '\n'.join(parts)


def format_predictions(predictions: list, changes: list) -> str:
    """æ ¼å¼åŒ–é¢„æµ‹éƒ¨åˆ†"""
    
    parts = []
    parts.append("## ğŸ¯ æœªæ¥é¢„æµ‹")
    parts.append("")
    
    timeframe_names = {
        "week": ("ğŸ“† æœªæ¥ä¸€å‘¨", "çŸ­æœŸ"),
        "month": ("ğŸ“† æœªæ¥ä¸€ä¸ªæœˆ", "ä¸­æœŸ"),
        "half_year": ("ğŸ“† æœªæ¥åŠå¹´", "ä¸­é•¿æœŸ"),
        "year": ("ğŸ“† æœªæ¥ä¸€å¹´", "é•¿æœŸ")
    }
    
    for timeframe, (title, desc) in timeframe_names.items():
        tf_predictions = [p for p in predictions if p.get('timeframe') == timeframe]
        tf_changes = {c['category']: c for c in changes if c.get('timeframe') == timeframe}
        
        if not tf_predictions:
            continue
        
        parts.append(f"### {title}")
        parts.append("")
        
        for pred in tf_predictions:
            cat = pred['category']
            info = CATEGORY_INFO.get(cat, {'name': cat, 'icon': 'ğŸ“Œ'})
            
            parts.append(f"**{info['icon']} {info['name']}**")
            parts.append("")
            
            # é¢„æµ‹å†…å®¹
            content = pred.get('content', '')[:200]
            parts.append(f"> {content}...")
            parts.append("")
            
            # å˜åŒ–è¯´æ˜
            if cat in tf_changes:
                change = tf_changes[cat]
                parts.append(f"â¬†ï¸ *å˜åŒ–: {change.get('reason', 'æ ¹æ®æœ€æ–°ä¿¡æ¯æ›´æ–°')}*")
                parts.append("")
        
        parts.append("---")
        parts.append("")
    
    return '\n'.join(parts)


def create_feishu_briefing(
    articles_by_category: dict,
    predictions: list,
    changes: list,
    date: datetime,
    folder_token: Optional[str] = None
) -> dict:
    """
    åˆ›å»ºæ ¼å¼ä¼˜åŒ–çš„é£ä¹¦ç®€æŠ¥æ–‡æ¡£
    
    Args:
        articles_by_category: æŒ‰åˆ†ç±»ç»„ç»‡çš„æ–‡ç«  {category: [article_dict, ...]}
        predictions: é¢„æµ‹åˆ—è¡¨ [{category, timeframe, content}, ...]
        changes: é¢„æµ‹å˜åŒ–åˆ—è¡¨
        date: ç®€æŠ¥æ—¥æœŸ
        folder_token: å¯é€‰çš„ç›®æ ‡æ–‡ä»¶å¤¹
    
    Returns:
        {'code': 0, 'data': {'url': '...'}} æˆ–é”™è¯¯ä¿¡æ¯
    """
    
    date_str = date.strftime("%Yå¹´%mæœˆ%dæ—¥ï¼ˆ%Aï¼‰")
    
    # æ„å»ºå®Œæ•´å†…å®¹
    content_parts = []
    
    # æ¦‚è§ˆ
    content_parts.append(format_overview(articles_by_category, date_str))
    
    # å„åˆ†ç±»æ–°é—»
    for category in ['ai', 'robotics', 'embodied_ai', 'semiconductor', 'auto', 
                     'health', 'economy', 'business', 'politics', 'investment',
                     'consumer_electronics', 'key_people']:
        articles = articles_by_category.get(category, [])
        if articles:
            content_parts.append(format_category_section(category, articles))
    
    # é¢„æµ‹
    if predictions:
        content_parts.append(format_predictions(predictions, changes))
    
    # é¡µè„š
    content_parts.append(f"*ç”Ÿæˆæ—¶é—´: {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}*")
    
    full_content = '\n'.join(content_parts)
    
    # åˆ›å»ºé£ä¹¦æ–‡æ¡£
    params = {
        'type': 'doc',
        'action': 'create',
        'title': f'æ¯æ—¥å…¨çƒç§‘æŠ€ç®€æŠ¥ - {date.strftime("%Y-%m-%d")}',
        'content': full_content
    }
    
    if folder_token:
        params['folder_token'] = folder_token
    
    result = feishu_operation(params)
    
    return result


def append_to_feishu_doc(
    doc_url: str,
    articles_by_category: dict,
    predictions: list,
    changes: list,
    date: datetime
) -> dict:
    """
    å‘å·²æœ‰é£ä¹¦æ–‡æ¡£è¿½åŠ å†…å®¹
    """
    
    date_str = date.strftime("%Yå¹´%mæœˆ%dæ—¥ï¼ˆ%Aï¼‰")
    
    # æ„å»ºè¿½åŠ å†…å®¹
    content_parts = []
    
    # æ—¥æœŸåˆ†éš”
    content_parts.append("")
    content_parts.append("---")
    content_parts.append("")
    content_parts.append(f"# ğŸ“… {date_str}")
    content_parts.append("")
    
    # æ¦‚è§ˆ
    content_parts.append(format_overview(articles_by_category, date_str))
    
    # å„åˆ†ç±»æ–°é—»
    for category in ['ai', 'robotics', 'embodied_ai', 'semiconductor', 'auto', 
                     'health', 'economy', 'business', 'politics', 'investment',
                     'consumer_electronics', 'key_people']:
        articles = articles_by_category.get(category, [])
        if articles:
            content_parts.append(format_category_section(category, articles))
    
    # é¢„æµ‹
    if predictions:
        content_parts.append(format_predictions(predictions, changes))
    
    full_content = '\n'.join(content_parts)
    
    # è¿½åŠ åˆ°æ–‡æ¡£
    result = feishu_operation({
        'type': 'doc',
        'action': 'append',
        'url': doc_url,
        'content': full_content
    })
    
    return result


# æµ‹è¯•
if __name__ == "__main__":
    # æ¨¡æ‹Ÿæ•°æ®
    test_articles = {
        'ai': [{
            'title_original': 'OpenAI Releases GPT-5',
            'title_zh': 'OpenAIå‘å¸ƒGPT-5',
            'source': 'TechCrunch',
            'published_at': '2026-02-13 09:30',
            'url': 'https://example.com',
            'summary_zh': 'è¿™æ˜¯ä¸€æ¡æµ‹è¯•æ–°é—»æ‘˜è¦...',
            'key_points': ['è¦ç‚¹1', 'è¦ç‚¹2'],
            'impact_analysis': 'å½±å“åˆ†æ...',
            'mentioned_people': ['Sam Altman']
        }]
    }
    
    test_predictions = [{
        'category': 'ai',
        'timeframe': 'week',
        'content': 'å…³æ³¨GPT-5åç»­å¸‚åœºååº”...'
    }]
    
    result = create_feishu_briefing(
        test_articles,
        test_predictions,
        [],
        datetime.now()
    )
    
    print(result)
